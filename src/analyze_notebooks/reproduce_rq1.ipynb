{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c80348d-9118-423e-a8b4-1ed65f65e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-lihao/mlbindings/src\n",
      "import facets_overview error!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "current_path = os.path.dirname(os.path.abspath(\"./\"))\n",
    "print(current_path)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "# Local helper modules\n",
    "import helper\n",
    "from helper import read_json, matchMLKeyWords\n",
    "import commonpath\n",
    "from commonpath import FIG_DIR, OUTPUT_DIR, PROJ_REPOS_CHUNKS_QA_DIR, PROJ_REPOS_PATH\n",
    "from collections import defaultdict\n",
    "\n",
    "# !pip install rapidfuzz\n",
    "# !pip install tldextract\n",
    "import tldextract\n",
    "from rapidfuzz import fuzz, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2a002-42d0-449d-97f6-901fc72e87fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7efcb0-8db0-42c2-9dc0-ac528e02a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specific order\n",
    "MODEL_ORDER = [\n",
    "    \"distilbert-base-uncased\", \"distilbert-base-cased\", \n",
    "    \"bert-base-uncased\", \"bert-large-uncased\", \"bert-base-cased\", \"bert-large-cased\",\n",
    "    \"albert-base-v2\", \"albert-large-v2\", \"albert-xlarge-v2\", \"albert-xxlarge-v2\",\n",
    "    \"roberta-base-squad2\", \"roberta-large-squad2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917862b8-8e3b-4a2d-a224-379f3eaf98f0",
   "metadata": {},
   "source": [
    "## Labelled dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cc5aa4-91a7-4ac5-87f9-670ba90dabff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- train set ----------\n",
      "1954 / 2346 = 83.29070758738278% False\n",
      "392 / 2346 = 16.70929241261722% True\n",
      "---------- validation set ----------\n",
      "50 / 100 = 50.0% True\n",
      "50 / 100 = 50.0% False\n",
      "---------- test set ----------\n",
      "50 / 100 = 50.0% True\n",
      "50 / 100 = 50.0% False\n"
     ]
    }
   ],
   "source": [
    "LABELLED_DATA = commonpath.DATA_DIR / \"labelled_data\"\n",
    "train_set_fp = LABELLED_DATA / \"binding_QA_train.csv\"\n",
    "validation_set_fp = LABELLED_DATA / \"binding_QA_validation.csv\"\n",
    "test_set_fp = LABELLED_DATA / \"binding_QA_test.csv\"\n",
    "\n",
    "LABLLED_GT = {\n",
    "    \"eval_predictions.json\": validation_set_fp,\n",
    "    \"predict_predictions.json\": test_set_fp,\n",
    "    \"test_results_binding.txt\": test_set_fp,\n",
    "}\n",
    "\n",
    "print(\"-\"*10, \"train set\", \"-\"*10)\n",
    "train_set_df = helper.readCSV(train_set_fp)\n",
    "helper.printValueCountsPercentage(train_set_df[\"is binding\"])\n",
    "\n",
    "print(\"-\"*10, \"validation set\", \"-\"*10)\n",
    "validation_set_df = helper.readCSV(validation_set_fp)\n",
    "helper.printValueCountsPercentage(validation_set_df[\"is binding\"])\n",
    "\n",
    "print(\"-\"*10, \"test set\", \"-\"*10)\n",
    "test_set_df = helper.readCSV(test_set_fp)\n",
    "helper.printValueCountsPercentage(test_set_df[\"is binding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a5b494-f1b4-4085-92a6-f5cdf4ee2843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_336184/3324452352.py:104: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = results_df._append(dataset_metrics, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>eval_f1-score</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>test_f1-score</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>qa_eval_f1</th>\n",
       "      <th>qa_eval_NoAns_f1</th>\n",
       "      <th>qa_eval_HasAns_exact</th>\n",
       "      <th>qa_eval_HasAns_f1</th>\n",
       "      <th>qa_test_f1</th>\n",
       "      <th>qa_test_NoAns_f1</th>\n",
       "      <th>qa_test_HasAns_exact</th>\n",
       "      <th>qa_test_HasAns_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-xlarge-v2</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.876404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert-large-cased</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.914667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.849333</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.767111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.822667</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert-large-uncased</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.921333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.862667</td>\n",
       "      <td>0.899667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.799333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distilbert-base-cased</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.897667</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>0.900857</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.801714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.901333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.822667</td>\n",
       "      <td>0.901333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.802667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.818000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.882048</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.824095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>roberta-large-squad2</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.934667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.869333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert-xxlarge-v2</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.933667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.887333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-squad2</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.934667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.889333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  eval_f1-score  eval_precision  eval_recall  \\\n",
       "0          albert-xlarge-v2       0.903226        0.976744         0.84   \n",
       "6          bert-large-cased       0.914894        0.977273         0.86   \n",
       "3         bert-base-uncased       0.905263        0.955556         0.86   \n",
       "9        bert-large-uncased       0.937500        0.978261         0.90   \n",
       "7     distilbert-base-cased       0.927835        0.957447         0.90   \n",
       "11          bert-base-cased       0.926316        0.977778         0.88   \n",
       "5   distilbert-base-uncased       0.918367        0.937500         0.90   \n",
       "4           albert-large-v2       0.947368        1.000000         0.90   \n",
       "2            albert-base-v2       0.937500        0.978261         0.90   \n",
       "10     roberta-large-squad2       0.948454        0.978723         0.92   \n",
       "8         albert-xxlarge-v2       0.916667        0.956522         0.88   \n",
       "1       roberta-base-squad2       0.960784        0.942308         0.98   \n",
       "\n",
       "    test_f1-score  test_precision  test_recall  qa_eval_f1  qa_eval_NoAns_f1  \\\n",
       "0        0.876404        1.000000         0.78    0.866667              0.98   \n",
       "6        0.936170        1.000000         0.88    0.914667              0.98   \n",
       "3        0.924731        1.000000         0.86    0.891333              0.96   \n",
       "9        0.924731        1.000000         0.86    0.921333              0.98   \n",
       "7        0.936170        1.000000         0.88    0.897667              0.96   \n",
       "11       0.947368        1.000000         0.90    0.901333              0.98   \n",
       "5        0.903226        0.976744         0.84    0.913333              0.94   \n",
       "4        0.924731        1.000000         0.86    0.923000              1.00   \n",
       "2        0.918367        0.937500         0.90    0.921000              0.98   \n",
       "10       0.969072        1.000000         0.94    0.943333              0.98   \n",
       "8        0.959184        0.979167         0.94    0.908000              0.96   \n",
       "1        0.969697        0.979592         0.96    0.943333              0.94   \n",
       "\n",
       "    qa_eval_HasAns_exact  qa_eval_HasAns_f1  qa_test_f1  qa_test_NoAns_f1  \\\n",
       "0                   0.68           0.753333    0.848000              1.00   \n",
       "6                   0.82           0.849333    0.883556              1.00   \n",
       "3                   0.78           0.822667    0.885000              1.00   \n",
       "9                   0.82           0.862667    0.899667              1.00   \n",
       "7                   0.74           0.835333    0.900857              1.00   \n",
       "11                  0.74           0.822667    0.901333              1.00   \n",
       "5                   0.86           0.886667    0.893000              0.98   \n",
       "4                   0.78           0.846000    0.909000              1.00   \n",
       "2                   0.78           0.862000    0.882048              0.94   \n",
       "10                  0.88           0.906667    0.934667              1.00   \n",
       "8                   0.80           0.856000    0.933667              0.98   \n",
       "1                   0.92           0.946667    0.934667              0.98   \n",
       "\n",
       "    qa_test_HasAns_exact  qa_test_HasAns_f1  \n",
       "0                   0.64           0.696000  \n",
       "6                   0.72           0.767111  \n",
       "3                   0.76           0.770000  \n",
       "9                   0.76           0.799333  \n",
       "7                   0.74           0.801714  \n",
       "11                  0.76           0.802667  \n",
       "5                   0.78           0.806000  \n",
       "4                   0.80           0.818000  \n",
       "2                   0.76           0.824095  \n",
       "10                  0.84           0.869333  \n",
       "8                   0.84           0.887333  \n",
       "1                   0.86           0.889333  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def report_classification_res(prediction_filepath, no_answer=\"\"):\n",
    "    with open(prediction_filepath, 'r') as file:\n",
    "        predictions = json.load(file)\n",
    "    predictions_df = pd.DataFrame.from_dict({\"id\": predictions.keys(), \"predicted_label\": predictions.values()})\n",
    "    predictions_df[\"id\"] = predictions_df[\"id\"].astype(int)\n",
    "\n",
    "    # Load actual labels\n",
    "    # prediction_filepath.name.startswith(\"eval\")\n",
    "    actual_labels_df = pd.read_csv(LABLLED_GT[prediction_filepath.name])\n",
    "    # print(len(actual_labels_df))\n",
    "    actual_labels_df = actual_labels_df[['id', 'answer']].rename(columns={'answer': 'actual_label'})\n",
    "    # print(sum(actual_labels_df[\"actual_label\"].isna()))\n",
    "    \n",
    "    # Merge predictions and actual labels\n",
    "    merged_df = pd.merge(predictions_df, actual_labels_df, on='id', how='inner')\n",
    "    # print(len(merged_df), len(predictions_df), len(actual_labels_df))\n",
    "    # merged_df\n",
    "    # 1. Binary Classification Performance\n",
    "    # Convert NaN in actual labels to 'no_answer' and non-NaN to 'has_answer'\n",
    "    merged_df['binary_actual'] = merged_df['actual_label'].apply(lambda x: 'not_binding' if pd.isna(x) else 'is_binding')\n",
    "    # Do the same for predicted labels, assuming that 'empty' represents no answer\n",
    "    # merged_df['binary_predicted'] = merged_df['predicted_label'].apply(lambda x: 'no_answer' if x == 'empty' else 'has_answer')\n",
    "    merged_df['binary_predicted'] = merged_df['predicted_label'].apply(lambda x: 'not_binding' if x == no_answer else 'is_binding')\n",
    "    \n",
    "    # Calculate binary classification report\n",
    "    binary_classification_report = classification_report(merged_df['binary_actual'], merged_df['binary_predicted'], digits=8)\n",
    "    # print(\"Binary Classification Report:\\n\", binary_classification_report)\n",
    "    binary_classification_report_dict = classification_report(merged_df['binary_actual'], merged_df['binary_predicted'], output_dict=True)\n",
    "    return binary_classification_report_dict\n",
    "    \n",
    "# Initialize an empty DataFrame to store results\n",
    "cols = [\"Model\", \"eval_f1-score\", \"eval_precision\", \"eval_recall\", \"test_f1-score\", \"test_precision\", \"test_recall\"]\n",
    "\n",
    "for dataset in [\"eval\", \"test\"]:\n",
    "    for metric in [\"f1\", \"NoAns_f1\", \"HasAns_exact\", \"HasAns_f1\"]:\n",
    "        cols.append(f\"qa_{dataset}_{metric}\")\n",
    "results_df = pd.DataFrame(columns=cols)\n",
    "# display(results_df)\n",
    "\n",
    "MODEL_PATH = commonpath.DATA_DIR / \"binding_qa\"\n",
    "best = {\n",
    "    \"model\": \"\",\n",
    "}\n",
    "compare_metric = \"eval_HasAns_f1\"\n",
    "best[compare_metric] = -1.0\n",
    "\n",
    "best_cls = {\n",
    "    \"model\": \"\",\n",
    "}\n",
    "compare_metric_cls = \"cls_eval_f1\"\n",
    "best_cls[compare_metric_cls] = -1.0\n",
    "\n",
    "for json_path in MODEL_PATH.rglob(\"trainer_state.json\"):\n",
    "    if \"checkpoint\" in str(json_path.parent):\n",
    "        continue\n",
    "    # all_files = list(p.iterdir())\n",
    "    # if len(all_files) == 1 and all_files[0].is_dir():\n",
    "    #     p = all_files[0]\n",
    "    # print(json_path.parent)\n",
    "    p = json_path.parent\n",
    "    # print(\"=\"*9, p, \"=\"*9)\n",
    "    dataset_metrics = {\"Model\": p.name}\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics_cls = {}\n",
    "    for dataset in [\"eval\", \"test\"]:\n",
    "        filepath = p / f\"{dataset}_results.json\"\n",
    "        if not filepath.exists():\n",
    "            if dataset != \"test\":\n",
    "                print(f\"{filepath} does not exist\")\n",
    "                continue\n",
    "                \n",
    "            filepath = p / f\"predict_results.json\"\n",
    "            if not filepath.exists():\n",
    "                print(f\"{filepath} does not exist\")\n",
    "                continue\n",
    "\n",
    "        res = read_json(filepath)\n",
    "        for metric in [\"f1\", \"NoAns_f1\", \"HasAns_exact\", \"HasAns_f1\"]:\n",
    "            key = f'{dataset}_{metric}'\n",
    "            value = res[key] / 100.0\n",
    "            metrics[key] = value\n",
    "            # print(f\"{key}: {value}\", end=\" \")\n",
    "            dataset_metrics[f\"qa_{key}\"] = value\n",
    "        # print()\n",
    "        \n",
    "        filepath = p / f\"{dataset}_predictions.json\"\n",
    "        if not filepath.exists():\n",
    "            if dataset != \"test\":\n",
    "                print(f\"{filepath} does not exist\")\n",
    "                continue\n",
    "                \n",
    "            filepath = p / f\"predict_predictions.json\"\n",
    "            if not filepath.exists():\n",
    "                print(f\"{filepath} does not exist\")\n",
    "                continue\n",
    "        classification_res = report_classification_res(filepath)\n",
    "        \n",
    "        for metric in [\"f1-score\", \"precision\", \"recall\"]:\n",
    "            dataset_metrics[f\"{dataset}_{metric}\"] = classification_res[\"is_binding\"][metric]\n",
    "        metrics_cls[f\"cls_{dataset}\"] = classification_res\n",
    "        metrics_cls[f\"cls_{dataset}_f1\"] = classification_res[\"is_binding\"][\"f1-score\"]\n",
    "        \n",
    "    results_df = results_df._append(dataset_metrics, ignore_index=True)\n",
    "    \n",
    "    if metrics and metrics[compare_metric] > best[compare_metric]:\n",
    "        for k, v in metrics.items():\n",
    "            best[k] = v\n",
    "        best[\"model\"] = p.name\n",
    "\n",
    "    if metrics_cls and metrics_cls[compare_metric_cls] > best_cls[compare_metric_cls]:\n",
    "        best_cls[compare_metric_cls] = metrics_cls[compare_metric_cls]\n",
    "        for k, v in metrics_cls.items():\n",
    "            best_cls[k] = v\n",
    "        best_cls[\"model\"] = p.name\n",
    "#     report_classification_res(p)\n",
    "results_df_qa = results_df\n",
    "results_df_qa.sort_values(\"qa_test_HasAns_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992a9cbb-f0cb-4d26-a00b-61f1778dcb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>eval_f1-score</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>test_f1-score</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>qa_eval_f1</th>\n",
       "      <th>qa_eval_NoAns_f1</th>\n",
       "      <th>qa_eval_HasAns_exact</th>\n",
       "      <th>qa_eval_HasAns_f1</th>\n",
       "      <th>qa_test_f1</th>\n",
       "      <th>qa_test_NoAns_f1</th>\n",
       "      <th>qa_test_HasAns_exact</th>\n",
       "      <th>qa_test_HasAns_f1</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distilbert-base-cased</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.897667</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>0.900857</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.801714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.822667</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert-large-uncased</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.921333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.862667</td>\n",
       "      <td>0.899667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.799333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>0.926316</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.901333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.822667</td>\n",
       "      <td>0.901333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.802667</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert-large-cased</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.914667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.849333</td>\n",
       "      <td>0.883556</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.767111</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.862000</td>\n",
       "      <td>0.882048</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.824095</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-xlarge-v2</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.876404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert-xxlarge-v2</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.933667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.887333</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-squad2</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.934667</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.889333</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>roberta-large-squad2</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.943333</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.934667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.869333</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  eval_f1-score  eval_precision  eval_recall  \\\n",
       "5   distilbert-base-uncased       0.918367        0.937500         0.90   \n",
       "7     distilbert-base-cased       0.927835        0.957447         0.90   \n",
       "3         bert-base-uncased       0.905263        0.955556         0.86   \n",
       "9        bert-large-uncased       0.937500        0.978261         0.90   \n",
       "11          bert-base-cased       0.926316        0.977778         0.88   \n",
       "6          bert-large-cased       0.914894        0.977273         0.86   \n",
       "2            albert-base-v2       0.937500        0.978261         0.90   \n",
       "4           albert-large-v2       0.947368        1.000000         0.90   \n",
       "0          albert-xlarge-v2       0.903226        0.976744         0.84   \n",
       "8         albert-xxlarge-v2       0.916667        0.956522         0.88   \n",
       "1       roberta-base-squad2       0.960784        0.942308         0.98   \n",
       "10     roberta-large-squad2       0.948454        0.978723         0.92   \n",
       "\n",
       "    test_f1-score  test_precision  test_recall  qa_eval_f1  qa_eval_NoAns_f1  \\\n",
       "5        0.903226        0.976744         0.84    0.913333              0.94   \n",
       "7        0.936170        1.000000         0.88    0.897667              0.96   \n",
       "3        0.924731        1.000000         0.86    0.891333              0.96   \n",
       "9        0.924731        1.000000         0.86    0.921333              0.98   \n",
       "11       0.947368        1.000000         0.90    0.901333              0.98   \n",
       "6        0.936170        1.000000         0.88    0.914667              0.98   \n",
       "2        0.918367        0.937500         0.90    0.921000              0.98   \n",
       "4        0.924731        1.000000         0.86    0.923000              1.00   \n",
       "0        0.876404        1.000000         0.78    0.866667              0.98   \n",
       "8        0.959184        0.979167         0.94    0.908000              0.96   \n",
       "1        0.969697        0.979592         0.96    0.943333              0.94   \n",
       "10       0.969072        1.000000         0.94    0.943333              0.98   \n",
       "\n",
       "    qa_eval_HasAns_exact  qa_eval_HasAns_f1  qa_test_f1  qa_test_NoAns_f1  \\\n",
       "5                   0.86           0.886667    0.893000              0.98   \n",
       "7                   0.74           0.835333    0.900857              1.00   \n",
       "3                   0.78           0.822667    0.885000              1.00   \n",
       "9                   0.82           0.862667    0.899667              1.00   \n",
       "11                  0.74           0.822667    0.901333              1.00   \n",
       "6                   0.82           0.849333    0.883556              1.00   \n",
       "2                   0.78           0.862000    0.882048              0.94   \n",
       "4                   0.78           0.846000    0.909000              1.00   \n",
       "0                   0.68           0.753333    0.848000              1.00   \n",
       "8                   0.80           0.856000    0.933667              0.98   \n",
       "1                   0.92           0.946667    0.934667              0.98   \n",
       "10                  0.88           0.906667    0.934667              1.00   \n",
       "\n",
       "    qa_test_HasAns_exact  qa_test_HasAns_f1  Rank  \n",
       "5                   0.78           0.806000     0  \n",
       "7                   0.74           0.801714     1  \n",
       "3                   0.76           0.770000     2  \n",
       "9                   0.76           0.799333     3  \n",
       "11                  0.76           0.802667     4  \n",
       "6                   0.72           0.767111     5  \n",
       "2                   0.76           0.824095     6  \n",
       "4                   0.80           0.818000     7  \n",
       "0                   0.64           0.696000     8  \n",
       "8                   0.84           0.887333     9  \n",
       "1                   0.86           0.889333    10  \n",
       "10                  0.84           0.869333    11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>qa_test_HasAns_f1</th>\n",
       "      <th>qa_test_HasAns_exact</th>\n",
       "      <th>test_f1-score</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>distilbert-base-cased</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert-large-uncased</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert-base-cased</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert-large-cased</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert-base-v2</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert-large-v2</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-xlarge-v2</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.876</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>albert-xxlarge-v2</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-squad2</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>roberta-large-squad2</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model qa_test_HasAns_f1 qa_test_HasAns_exact  \\\n",
       "5   distilbert-base-uncased             0.806                0.780   \n",
       "7     distilbert-base-cased             0.802                0.740   \n",
       "3         bert-base-uncased             0.770                0.760   \n",
       "9        bert-large-uncased             0.799                0.760   \n",
       "11          bert-base-cased             0.803                0.760   \n",
       "6          bert-large-cased             0.767                0.720   \n",
       "2            albert-base-v2             0.824                0.760   \n",
       "4           albert-large-v2             0.818                0.800   \n",
       "0          albert-xlarge-v2             0.696                0.640   \n",
       "8         albert-xxlarge-v2             0.887                0.840   \n",
       "1       roberta-base-squad2             0.889                0.860   \n",
       "10     roberta-large-squad2             0.869                0.840   \n",
       "\n",
       "   test_f1-score test_precision test_recall  \n",
       "5          0.903          0.977       0.840  \n",
       "7          0.936          1.000       0.880  \n",
       "3          0.925          1.000       0.860  \n",
       "9          0.925          1.000       0.860  \n",
       "11         0.947          1.000       0.900  \n",
       "6          0.936          1.000       0.880  \n",
       "2          0.918          0.938       0.900  \n",
       "4          0.925          1.000       0.860  \n",
       "0          0.876          1.000       0.780  \n",
       "8          0.959          0.979       0.940  \n",
       "1          0.970          0.980       0.960  \n",
       "10         0.969          1.000       0.940  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_qa[\"Rank\"] = results_df_qa[\"Model\"].apply(MODEL_ORDER.index)\n",
    "results_df_qa = results_df_qa.sort_values(\"Rank\")\n",
    "display(results_df_qa)\n",
    "\n",
    "\n",
    "def formatDataFrame(df, columns, digits=3):\n",
    "    data = df[columns].copy()\n",
    "    format_str = \"{:.\" + str(digits) + \"f}\"\n",
    "    for c in columns:\n",
    "        data[c] = data[c].apply(lambda x: format_str.format(round(x, digits)) if isinstance(x, float) else x)\n",
    "    return data\n",
    "\n",
    "formatDataFrame(results_df_qa, [\"Model\", \"qa_test_HasAns_f1\", \"qa_test_HasAns_exact\", \"test_f1-score\", \"test_precision\", \"test_recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f657da2-0f54-4ded-a366-df6b51bffec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rq2_ml_bindings = pd.read_csv(commonpath.DATA_DIR / \"rq2_ml_bindings.csv\")\n",
    "rq2_ml_bindings = helper.formatIDColumnsToStr(rq2_ml_bindings)\n",
    "rq2_ml_repos = pd.read_csv(commonpath.DATA_DIR / \"rq2_ml_repos.csv\")\n",
    "rq2_ml_repos = helper.formatIDColumnsToStr(rq2_ml_repos)\n",
    "rq2_ml_bindings[\"Host Repo IDs\"] = rq2_ml_bindings[\"Host Repo IDs\"].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef83ee29-9194-4b86-a002-767b76da4de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2347 bindings\n",
      "546 ML repos\n",
      "546 / 11763 = 4.641673042591176% \n",
      "Index(['Unnamed: 0', 'ID', 'Platform', 'Name', 'Description', 'Keywords',\n",
      "       'Licenses', 'Repository URL', 'Versions Count',\n",
      "       'Dependent Projects Count', 'Status', 'Dependent Repositories Count',\n",
      "       'Repository ID', 'Repository Host Type', 'Repository Name with Owner',\n",
      "       'Repository Description', 'Repository Fork?', 'Repository Stars Count',\n",
      "       'Repository License', 'Repository Status', 'Repository Keywords',\n",
      "       'Binding Host', 'Homepage URL', 'Homepage Domain',\n",
      "       'Binding Host Normalized', 'Binding Host Normalized - No Space',\n",
      "       'Host Repo IDs', 'is official', 'Type'],\n",
      "      dtype='object')\n",
      "2206 / 2347 = 93.992330634853% community\n",
      "78 / 2347 = 3.3233915636983387% same_repo\n",
      "53 / 2347 = 2.258201959948871% same_repo_owner\n",
      "7 / 2347 = 0.2982530890498509% same_repo_url_owner\n",
      "3 / 2347 = 0.1278227524499361% same_homepage_url\n",
      "Community maintained:\n",
      "2206 / 2347 = 93.992330634853% True\n",
      "141 / 2347 = 6.007669365146996% False\n",
      "Amoung official maintained - same repo\n",
      "78 / 141 = 55.319148936170215% True\n",
      "63 / 141 = 44.680851063829785% False\n"
     ]
    }
   ],
   "source": [
    "ml_repos = pd.read_csv(commonpath.DATA_DIR / \"not_toy_ml_repos.csv\")\n",
    "ml_repos = helper.formatIDColumnsToStr(ml_repos)\n",
    "# filter out non programming languages\n",
    "ml_repos = ml_repos[~ml_repos[\"Language\"].isin([\"Jupyter Notebook\", \"HTML\"])]\n",
    "\n",
    "print(len(rq2_ml_bindings[\"ID\"].unique()), \"bindings\")\n",
    "print(len(rq2_ml_repos[\"ID\"].unique()), \"ML repos\")\n",
    "helper.printPercentage(len(rq2_ml_repos[\"ID\"].unique()), len(ml_repos))\n",
    "\n",
    "def isOfficial(df):\n",
    "    host_repo_ids = df[\"Host Repo IDs\"]\n",
    "    if df[\"Repository ID\"] in host_repo_ids:\n",
    "        return \"same_repo\"\n",
    "    \n",
    "    host_repo_info = rq2_ml_repos[rq2_ml_repos[\"ID\"].isin(host_repo_ids)]\n",
    "    same_repo_mask = host_repo_info[\"Name with Owner\"] == df[\"Repository Name with Owner\"]\n",
    "    if sum(same_repo_mask) > 0:\n",
    "        return \"same_repo_name\"\n",
    "    \n",
    "    host_owners = host_repo_info[\"Name with Owner\"].apply(lambda s: s.split(\"/\")[0])\n",
    "    if isinstance(df[\"Repository Name with Owner\"], str):\n",
    "        same_repo_owner_mask = host_owners == df[\"Repository Name with Owner\"].split(\"/\")[0]\n",
    "        if sum(same_repo_owner_mask) > 0:\n",
    "            return \"same_repo_owner\"\n",
    "\n",
    "    if isinstance(df[\"Repository URL\"], str):\n",
    "        if \"//bitbucket.org/\" in df[\"Repository URL\"] or \"//gitlab.com/\" in df[\"Repository URL\"] or \"//github.com/\" in df[\"Repository URL\"]:\n",
    "            repository_owner = df[\"Repository URL\"].split(\"/\")[-2]\n",
    "            if sum(host_owners == repository_owner):\n",
    "                return \"same_repo_url_owner\"\n",
    "    \n",
    "    if isinstance(df[\"Homepage URL\"], str):\n",
    "        if sum(df[\"Homepage URL\"] == host_repo_info[\"Homepage URL\"]) > 0:\n",
    "            return \"same_homepage_url\"\n",
    "    return \"community\"\n",
    "\n",
    "\n",
    "print(rq2_ml_bindings.columns)\n",
    "rq2_ml_bindings_is_official = rq2_ml_bindings.apply(isOfficial, axis=1)\n",
    "rq2_ml_bindings[\"is official\"] = rq2_ml_bindings_is_official\n",
    "helper.printValueCountsPercentage(rq2_ml_bindings_is_official)\n",
    "print(\"Community maintained:\")\n",
    "helper.printValueCountsPercentage(rq2_ml_bindings_is_official == \"community\")\n",
    "print(\"Amoung official maintained - same repo\")\n",
    "helper.printValueCountsPercentage(rq2_ml_bindings_is_official[rq2_ml_bindings_is_official != \"community\"] == \"same_repo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbindings_reproduce",
   "language": "python",
   "name": "mlbindings_reproduce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
